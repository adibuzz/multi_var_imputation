{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98bd749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark import SparkConf\n",
    "import pyarrow\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "conf=SparkConf()\n",
    "conf.set(\"spark.executor.memory\", \"50g\")\n",
    "conf.set(\"spark.driver.memory\", \"50g\")\n",
    "conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.ansi.enabled\", \"false\")\n",
    "conf.set(\"spark.driver.maxResultSize\", \"0\")  # Disable driver result size limit\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"100000\")  # Adjust batch size for Arrow\n",
    "conf.set(\"spark.sql.execution.arrow.fallback.enabled\", \"true\")  # Enable fallback to Pandas if Arrow fails\n",
    "\n",
    "# Read individual CSV files into Spark DataFrames\n",
    "\n",
    "# admissions = spark.read.csv('./demo_tables/admissions.csv', header=True)\n",
    "# patients = spark.read.csv('./demo_tables/patients.csv', header=True)\n",
    "# chartevents = spark.read.csv('./demo_tables/chartevents.csv', header=True)\n",
    "# icustays = spark.read.csv('./demo_tables/icustays.csv', header=True)\n",
    "# diagnoses_icd = spark.read.csv('./demo_tables/diagnoses_icd.csv', header=True)\n",
    "# d_icd_diagnoses = spark.read.csv('./demo_tables/d_icd_diagnoses.csv', header=True)\n",
    "\n",
    "# Read individual large CSV files into Spark DataFrames\n",
    "admissions = spark.read.csv('./full_tables/admissions.csv', header=True)\n",
    "chartevents = spark.read.csv('./full_tables/chartevents.csv', header=True)\n",
    "patients = spark.read.csv('./full_tables/patients.csv', header=True)\n",
    "icustays = spark.read.csv('./full_tables/icustays.csv', header=True)\n",
    "diagnoses_icd = spark.read.csv('./full_tables/diagnoses_icd.csv', header=True)\n",
    "d_icd_diagnoses = spark.read.csv('./full_tables/d_icd_diagnoses.csv', header=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Register the DataFrame 'data' as a temporary view for use in SQL queries\n",
    "admissions.createOrReplaceTempView(\"ADMISSIONS\")\n",
    "chartevents.createOrReplaceTempView(\"CHARTEVENTS\")\n",
    "patients.createOrReplaceTempView(\"PATIENTS\")\n",
    "diagnoses_icd.createOrReplaceTempView(\"DIAGNOSES_ICD\")\n",
    "d_icd_diagnoses.createOrReplaceTempView(\"D_ICD_DIAGNOSES\")\n",
    "icustays.createOrReplaceTempView(\"ICUSTAYS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abca33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH d_calc AS (\n",
    "    SELECT\n",
    "        SUBJECT_ID,\n",
    "        CAST(intime AS timestamp) AS intime,\n",
    "        CAST(outtime AS timestamp) AS outtime,\n",
    "        row_number() OVER (PARTITION BY SUBJECT_ID ORDER BY CAST(intime AS timestamp) ASC) AS record_seq,\n",
    "        HADM_ID,\n",
    "        STAY_ID,\n",
    "        LAG(CAST(outtime AS timestamp), 1) OVER (\n",
    "            PARTITION BY SUBJECT_ID\n",
    "            ORDER BY CAST(intime AS timestamp)\n",
    "        ) AS previous_outtime\n",
    "    FROM ICUSTAYS\n",
    "),\n",
    "d_days AS (\n",
    "    SELECT\n",
    "        SUBJECT_ID,\n",
    "        HADM_ID,\n",
    "        STAY_ID,\n",
    "        CAST(intime AS double) - CAST(previous_outtime AS double) AS Duration,\n",
    "        record_seq\n",
    "    FROM d_calc\n",
    "    WHERE previous_outtime IS NOT NULL\n",
    "),\n",
    "d_days_filtered_subject_id AS (\n",
    "    SELECT\n",
    "        SUBJECT_ID AS SUB_ID,\n",
    "        HADM_ID AS H_ID,\n",
    "        STAY_ID AS S_ID,\n",
    "        Duration / 86400 AS LoS,\n",
    "        record_seq\n",
    "    FROM d_days\n",
    "    WHERE SUBJECT_ID IN (SELECT SUBJECT_ID FROM d_days WHERE record_seq = 2)\n",
    "      AND record_seq <= 2\n",
    "    ORDER BY SUBJECT_ID\n",
    "),\n",
    "HADM_IDs AS (\n",
    "    SELECT *\n",
    "    FROM d_days_filtered_subject_id\n",
    "    WHERE record_seq = 1\n",
    "),\n",
    "chartevents_filtered AS (\n",
    "    SELECT *\n",
    "    FROM chartevents\n",
    "    WHERE ITEMID IN (220546, 224828, 220644, 220235, 225624, 229761, \n",
    "      220363, 220422, 220467, 220339, 224696, 225170, 227466, 220050,\n",
    "         220051, 220052, 228386, 220074, 220367, 220045, 225651, 226754,\n",
    "                226755, 227015, 227467, 220451, 224697, 225168, 220210,\n",
    "                    220227, 223761, 225690, 220650)\n",
    "        AND SUBJECT_ID IN (SELECT SUB_ID FROM d_days_filtered_subject_id)\n",
    ")\n",
    "SELECT *\n",
    "FROM (\n",
    "    SELECT *,\n",
    "           unix_timestamp(charttime) AS chart_ts,\n",
    "           MAX(unix_timestamp(charttime)) OVER (PARTITION BY SUBJECT_ID) AS max_chart_ts\n",
    "    FROM chartevents_filtered\n",
    ")\n",
    "WHERE chart_ts >= max_chart_ts - 48*3600\n",
    "ORDER BY SUBJECT_ID, charttime\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(query)\n",
    "\n",
    "# Save each SUBJECT_ID as a separate file (e.g., CSV)\n",
    "subject_ids = [row.subject_id for row in result.select(\"subject_id\").distinct().collect()]\n",
    "print(f\"Number of unique SUBJECT_IDs: {len(subject_ids)}\")\n",
    "\n",
    "\n",
    "output_base_path = \"./Readmitted_patients\"\n",
    "\n",
    "for subject_id in subject_ids:\n",
    "    print(f\"Processing SUBJECT_ID: {subject_id}\")\n",
    "\n",
    "    # Filter the result for the current subject_id\n",
    "    filtered_df = result.filter(result.subject_id == subject_id)\n",
    "    # Remove null values in the 'valuenum' column\n",
    "    filtered_df = filtered_df.filter(filtered_df.valuenum.isNotNull())\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_base_path, exist_ok=True)\n",
    "\n",
    "    # Define output path for this subject_id\n",
    "    output_path = f\"{output_base_path}/patient_{subject_id}\"\n",
    "\n",
    "    filtered_df.toPandas().to_parquet(f'{output_path}.parquet', index=False)\n",
    "\n",
    "\n",
    "# Convert to py file from notebook\n",
    "print(f\"Data for {len(subject_ids)} patients has been saved to {output_base_path} in Parquet format.\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47579399",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().system('jupyter nbconvert --to python test.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
